#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Mon Dec 11 13:58:12 2017

@author: CW
"""



import math
import numpy as np

import torch
import torch.nn as nn
from torch.nn import Module
from torch.nn import functional as F
from torch.nn.parameter import Parameter
from torch.nn.modules.utils import _pair
from torch.autograd import Variable

# aliasing
N_ = None


delta = 1e-6
softplus_ = nn.Softplus()
softplus = lambda x: softplus_(x) + delta 
sigmoid_ = nn.Sigmoid()
sigmoid = lambda x: sigmoid_(x) * (1-delta) + 0.5 * delta 
sigmoid2 = lambda x: sigmoid(x) * 2.0
logsigmoid = lambda x: -softplus(-x)
logit = lambda x: torch.log
log = lambda x: torch.log(x*1e2)-np.log(1e2)
logit = lambda x: log(x) - log(1-x)
def softmax(x, dim=-1):
    e_x = torch.exp(x - x.max(dim=dim, keepdim=True)[0])
    out = e_x / e_x.sum(dim=dim, keepdim=True)
    return out

sum1 = lambda x: x.sum(1)
sum_from_one = lambda x: sum_from_one(sum1(x)) if len(x.size())>2 else sum1(x)
    
class WNlinear(Module):

    def __init__(self, in_features, out_features, 
                 bias=True, mask=N_, norm=True):
        super(WNlinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.register_buffer('mask',mask)
        self.norm = norm
        self.direction = Parameter(torch.Tensor(out_features, in_features))
        self.scale = Parameter(torch.Tensor(out_features))
        if bias:
            self.bias = Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', N_)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.direction.size(1))
        self.direction.data.uniform_(-stdv, stdv)
        self.scale.data.uniform_(1, 1)
        if self.bias is not N_:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input):
        if self.norm:
            dir_ = self.direction
            direction = dir_.div(dir_.pow(2).sum(1).sqrt()[:,N_])
            weight = self.scale[:,N_].mul(direction)
        else:
            weight = self.scale[:,N_].mul(self.direction)
        if self.mask is not N_:
            #weight = weight * getattr(self.mask, 
            #                          ('cpu', 'cuda')[weight.is_cuda])()
            weight = weight * Variable(self.mask)
        return F.linear(input, weight, self.bias)

    def __repr__(self):
        return self.__class__.__name__ + '(' \
            + 'in_features=' + str(self.in_features) \
            + ', out_features=' + str(self.out_features) + ')'




class CWNlinear(Module):

    def __init__(self, in_features, out_features, context_features,
                 mask=N_, norm=True):
        super(CWNlinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.context_features = context_features
        self.register_buffer('mask',mask)
        self.norm = norm
        self.direction = Parameter(torch.Tensor(out_features, in_features))
        self.cscale = nn.Linear(context_features, out_features)
        self.cbias = nn.Linear(context_features, out_features)
        self.reset_parameters()
        self.cscale.weight.data.normal_(0, 0.001)
        self.cbias.weight.data.normal_(0, 0.001)

    def reset_parameters(self):
        self.direction.data.normal_(0, 0.001)
        
    def forward(self, inputs):
        input, context = inputs
        scale = self.cscale(context)
        bias = self.cbias(context)
        if self.norm:
            dir_ = self.direction
            direction = dir_.div(dir_.pow(2).sum(1).sqrt()[:,N_])
            weight = direction
        else:
            weight = self.direction
        if self.mask is not N_:
            #weight = weight * getattr(self.mask,
            #                          ('cpu', 'cuda')[weight.is_cuda])()
            weight = weight * Variable(self.mask)
        return scale * F.linear(input, weight, None) + bias, context

    def __repr__(self):
        return self.__class__.__name__ + '(' \
            + 'in_features=' + str(self.in_features) \
            + ', out_features=' + str(self.out_features) + ')'



      
class WNBilinear(Module):
    
    def __init__(self, in1_features, in2_features, out_features, bias=True):
        super(WNBilinear, self).__init__()
        self.in1_features = in1_features
        self.in2_features = in2_features
        self.out_features = out_features
        self.direction = Parameter(torch.Tensor(
                out_features, in1_features, in2_features))
        self.scale = Parameter(torch.Tensor(out_features))
        if bias:
            self.bias = Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', N_)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.direction.size(1))
        self.direction.data.uniform_(-stdv, stdv)
        self.scale.data.uniform_(1, 1)
        if self.bias is not N_:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input1, input2):
        dir_ = self.direction
        direction = dir_.div(dir_.pow(2).sum(1).sum(1).sqrt()[:,N_,N_])
        weight = self.scale[:,N_,N_].mul(direction)
        return F.bilinear(input1, input2, weight, self.bias)

    def __repr__(self):
        return self.__class__.__name__ + '(' \
            + 'in1_features=' + str(self.in1_features) \
            + ', in2_features=' + str(self.in2_features) \
            + ', out_features=' + str(self.out_features) + ')'


                                           
class _WNconvNd(Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride,
                 padding, dilation, transposed, output_padding, groups, bias):
        super(_WNconvNd, self).__init__()
        if in_channels % groups != 0:
            raise ValueError('in_channels must be divisible by groups')
        if out_channels % groups != 0:
            raise ValueError('out_channels must be divisible by groups')
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.transposed = transposed
        self.output_padding = output_padding
        self.groups = groups
        
        # weight â€“ filters tensor (out_channels x in_channels/groups x kH x kW)
        if transposed:
            self.direction = Parameter(torch.Tensor(
                    in_channels, out_channels // groups, *kernel_size))
            self.scale = Parameter(torch.Tensor(in_channels))
        else:
            self.direction = Parameter(torch.Tensor(
                    out_channels, in_channels // groups, *kernel_size))
            self.scale = Parameter(torch.Tensor(out_channels))
        if bias:
            self.bias = Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', N_)
        self.reset_parameters()

    def reset_parameters(self):
        n = self.in_channels
        for k in self.kernel_size:
            n *= k
        stdv = 1. / math.sqrt(n)
        self.direction.data.uniform_(-stdv, stdv)
        self.scale.data.uniform_(1, 1)
        if self.bias is not N_:
            self.bias.data.uniform_(-stdv, stdv)

    def __repr__(self):
        s = ('{name} ({in_channels}, {out_channels}, kernel_size={kernel_size}'
             ', stride={stride}')
        if self.padding != (0,) * len(self.padding):
            s += ', padding={padding}'
        if self.dilation != (1,) * len(self.dilation):
            s += ', dilation={dilation}'
        if self.output_padding != (0,) * len(self.output_padding):
            s += ', output_padding={output_padding}'
        if self.groups != 1:
            s += ', groups={groups}'
        if self.bias is N_:
            s += ', bias=False'
        s += ')'
        return s.format(name=self.__class__.__name__, **self.__dict__)


class WNconv2d(_WNconvNd):

    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=0, dilation=1, groups=1, bias=True,
                 mask=N_, norm=True):
        kernel_size = _pair(kernel_size)
        stride = _pair(stride)
        padding = _pair(padding)
        dilation = _pair(dilation)
        super(WNconv2d, self).__init__(
            in_channels, out_channels, kernel_size, stride, padding, dilation,
            False, _pair(0), groups, bias)
        
        self.register_buffer('mask',mask)
        self.norm = norm
        
    def forward(self, input):
        if self.norm:
            dir_ = self.direction
            direction = dir_.div(
                    dir_.pow(2).sum(1).sum(1).sum(1).sqrt()[:,N_,N_,N_])
            weight = self.scale[:,N_,N_,N_].mul(direction)
        else:
            weight = self.scale[:,N_,N_,N_].mul(self.direction)
        if self.mask is not None:
            weight = weight * Variable(self.mask)
        return F.conv2d(input, weight, self.bias, self.stride,
                        self.padding, self.dilation, self.groups)



class ResConv2d(nn.Module):
    
    def __init__(
            self, in_channels, out_channels, kernel_size, stride=1, 
            padding=0, dilation=1, groups=1, bias=True, activation=nn.ReLU(),
            oper=WNconv2d):
        super(ResConv2d, self).__init__()
        
        self.conv_0h = oper(
                in_channels, out_channels, kernel_size, stride, 
                padding, dilation, groups, bias)
        self.conv_h1 = oper(
                out_channels, out_channels, 3, 1, 1, 1, 1, True)
        self.conv_01 = oper(
                in_channels, out_channels, kernel_size, stride, 
                padding, dilation, groups, bias)
        
        self.activation = activation

    def forward(self, input):
        h = self.activation(self.conv_0h(input))
        out_nonlinear = self.conv_h1(h)
        out_skip = self.conv_01(input)
        return out_nonlinear + out_skip


class ResLinear(nn.Module):
    
    def __init__(
            self, in_features, out_features, bias=True, same_dim=False,
            activation=nn.ReLU(), oper=WNlinear):
        super(ResLinear, self).__init__()
        
        self.same_dim = same_dim
        
        self.dot_0h = oper(in_features, out_features, bias)
        self.dot_h1 = oper(out_features, out_features, bias)
        if not same_dim:
            self.dot_01 = oper(in_features, out_features, bias)
        
        self.activation = activation
        
    def forward(self, input):
        h = self.activation(self.dot_0h(input))
        out_nonlinear = self.dot_h1(h)
        out_skip = input if self.same_dim else self.dot_01(input)
        return out_nonlinear + out_skip

        
    
class Reshape(nn.Module):
    
    def __init__(self, shape):
        super(Reshape, self).__init__()
        self.shape = shape
        
    def forward(self, input):
        return input.view(self.shape)


class Slice(nn.Module):
    
    def __init__(self, slc):
        super(Slice, self).__init__()
        self.slc = slc
        
    def forward(self, input):
        return input.__getitem__(self.slc)
        
        
class SliceFactory(object):
    def __init__(self):
        pass
    
    def __getitem__(self, slc):
        return Slice(slc)
slicer = SliceFactory()


class Lambda(nn.Module):
    
    def __init__(self, function):
        super(Lambda, self).__init__()
        self.function = function
        
    def forward(self, input):
        return self.function(input)


class SequentialFlow(nn.Sequential):
    
    def sample(self, n=1, context=None):
        dim = self[0].dim
        if isinstance(dim, int):
            dim = [dim,]
        
        spl = torch.autograd.Variable(torch.FloatTensor(n,*dim).normal_())
        lgd = torch.autograd.Variable(torch.from_numpy(
            np.random.rand(n).astype('float32')))
        if context is None:
            context = torch.autograd.Variable(torch.from_numpy(
            np.zeros((n,self[0].context_dim)).astype('float32')))
            
        if hasattr(self, 'gpu'):
            if self.gpu:
                spl = spl.cuda()
                lgd = lgd.cuda()
                context = context.gpu()
        
        return self.forward((spl, lgd, context))
    
    
    
    def cuda(self):
        self.gpu = True
        return super(SequentialFlow, self).cuda()




if __name__ == '__main__':
    
    mdl = CWNlinear(2,5,3)
    
    
    inp = torch.autograd.Variable(
            torch.from_numpy(np.random.rand(2,2).astype('float32')))
    con = torch.autograd.Variable(
            torch.from_numpy(np.random.rand(2,3).astype('float32')))
    
    print mdl((inp, con))[0].size()
    
